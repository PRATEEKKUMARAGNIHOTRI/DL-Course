{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQoFU913fLwX",
        "colab_type": "text"
      },
      "source": [
        "<h1>ASSIGNMENT1</h1><h2>Download modules</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnpXuU-cT8wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install SimpleITK\n",
        "!pip install PyDrive\n",
        "!pip install nilearn\n",
        "!pip install git+https://www.github.com/farizrahman4u/keras-contrib.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipSJN5Pqf_mW",
        "colab_type": "text"
      },
      "source": [
        "<h2>Import modules</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lp1wOIaTGBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import subprocess\n",
        "import zipfile\n",
        "from functools import partial\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, LeakyReLU, Add, SpatialDropout3D,Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Deconvolution3D\n",
        "from keras.engine import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nilearn.image.image import check_niimg\n",
        "from nilearn.image.image import _crop_img_to as crop_img_to\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from keras_contrib.layers.normalization import instancenormalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqDLiLexMcA",
        "colab_type": "text"
      },
      "source": [
        "<h2>Tunneling to TENSORBOARD</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBJ-6d3xH-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__all__ = [\n",
        "  'install_ngrok', \n",
        "  'launch_tensorboard',\n",
        "]\n",
        "\n",
        "def __shell__(cmd, split=True):\n",
        "  # get_ipython().system_raw(cmd)\n",
        "  result = get_ipython().getoutput(cmd, split=split)\n",
        "  if result and not split:\n",
        "    result = result.strip('\\n')\n",
        "  return result  \n",
        "\n",
        "\n",
        "# tested OK\n",
        "def install_ngrok(bin_dir=\"/tmp\"):\n",
        "  \"\"\" download and install ngrok on local vm instance\n",
        "  Args:\n",
        "    bin_dir: full path for the target directory for the `ngrok` binary\n",
        "  \"\"\"\n",
        "  TARGET_DIR = bin_dir\n",
        "  CWD = os.getcwd()\n",
        "  is_grok_avail = os.path.isfile(os.path.join(TARGET_DIR,'ngrok'))\n",
        "  if is_grok_avail:\n",
        "    print(\"ngrok installed\")\n",
        "  else:\n",
        "    import platform\n",
        "    plat = platform.platform() # 'Linux-4.4.64+-x86_64-with-Ubuntu-17.10-artful'\n",
        "    if 'x86_64' in plat:\n",
        "      \n",
        "      os.chdir('/tmp')\n",
        "      print(\"calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\" )\n",
        "      get_ipython().system_raw( \"wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\" )\n",
        "      print(\"calling unzip ngrok-stable-linux-amd64.zip ...\")\n",
        "      get_ipython().system_raw( \"unzip ngrok-stable-linux-amd64.zip\" )\n",
        "      os.rename(\"ngrok\", \"{}/ngrok\".format(TARGET_DIR))\n",
        "      os.remove(\"ngrok-stable-linux-amd64.zip\")\n",
        "      is_grok_avail = os.path.isfile(os.path.join(TARGET_DIR,'ngrok'))\n",
        "      os.chdir(TARGET_DIR)\n",
        "      if is_grok_avail:\n",
        "        print(\"ngrok installed. path={}\".format(os.path.join(TARGET_DIR,'ngrok')))\n",
        "      else:\n",
        "        # ValueError: ERROR: ngrok not found, path=\n",
        "        raise ValueError( \"ERROR: ngrok not found, path=\".format(TARGET_DIR) )\n",
        "    else:\n",
        "      raise NotImplementedError( \"ERROR, ngrok install not configured for this platform, platform={}\".format(plat))\n",
        "    os.chdir(CWD)\n",
        "    return\n",
        "    \n",
        "# tested OK\n",
        "def launch_tensorboard(bin_dir=\"/tmp\", log_dir=\"/tmp\", retval=False):\n",
        "  \"\"\"returns a public tensorboard url based on the ngrok package\n",
        "  checks if `ngrok` is available, and installs, if necessary, to `bin_dir`\n",
        "  launches tensorboard, if necessary\n",
        "  see: https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab\n",
        "  Args:\n",
        "    bin_dir: full path for the target directory for the `ngrok` binary\n",
        "    log_dir: full path for the tensorflow `log_dir`\n",
        "  Return:\n",
        "    public url for tensorboard if retval==True\n",
        "      NOTE: the method will print a link to stdout (cell output) for the tensorflow URL. \n",
        "      But the link printed from the return value has an extra \"%27\" in the URL which causes an error\n",
        "  \"\"\"\n",
        "  install_ngrok(bin_dir)\n",
        "    \n",
        "  if not tf.gfile.Exists(log_dir):  tf.gfile.MakeDirs(log_dir)\n",
        "  \n",
        "  # check status of tensorboard and ngrok\n",
        "  ps = __shell__(\"ps -ax\")\n",
        "  is_tensorboard_running = len([f for f in ps if \"tensorboard\" in f ]) > 0\n",
        "  is_ngrok_running = len([f for f in ps if \"ngrok\" in f ]) > 0\n",
        "  print(\"status: tensorboard={}, ngrok={}\".format(is_tensorboard_running, is_ngrok_running))\n",
        "\n",
        "  if not is_tensorboard_running:\n",
        "    get_ipython().system_raw(\n",
        "        'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "        .format(log_dir)\n",
        "    )\n",
        "    is_tensorboard_running = True\n",
        "    \n",
        "  if not is_ngrok_running:  \n",
        "    #    grok should be installed in /tmp/ngrok\n",
        "    get_ipython().system_raw('{}/ngrok http 6006 &'.format(bin_dir))\n",
        "    is_ngrok_running = True\n",
        "\n",
        "  # get tensorboard url\n",
        "  # BUG: getting connection refused for HTTPConnectionPool(host='localhost', port=4040)\n",
        "  #     on first run, retry works\n",
        "  import time\n",
        "  time.sleep(3)\n",
        "  retval = requests.get('http://localhost:4040/api/tunnels')\n",
        "  tensorboard_url = retval.json()['tunnels'][0]['public_url'].strip()\n",
        "  print(\"tensorboard url=\", tensorboard_url)\n",
        "  if retval:\n",
        "    return tensorboard_url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGWw4fkUxLfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set paths\n",
        "ROOT = %pwd\n",
        "LOG_DIR = os.path.join(ROOT, 'log1')\n",
        "\n",
        "# will install `ngrok`, if necessary\n",
        "# will create `log_dir` if path does not exist\n",
        "launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbsCKFDIxuFD",
        "colab_type": "text"
      },
      "source": [
        "<h2> Unzip in google drive</h2>(NOT IMPORTANT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Y678Hws8mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drive.mount(\"/content/drive\")\n",
        "# with zipfile.ZipFile(\"/content/drive/My Drive/BRATS2015_Training.zip\",\"r\") as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/drive/My Drive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEWn4DHK3iTC",
        "colab_type": "text"
      },
      "source": [
        "<h2>Local Unzip</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC6tzNZwlmIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## connect to drive and Locally download the file\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "download = drive.CreateFile({'id': '1AFiE99KilM3w9-f3BzhFSN_3_ztF5KTK'})\n",
        "download.GetContentFile('BRATS2015.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wugwEiwmF1dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Locally extract the zip\n",
        "\n",
        "with zipfile.ZipFile(\"./BRATS2015.tar\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./BRATS2015\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NBMCojze5mq",
        "colab_type": "text"
      },
      "source": [
        "<h2>All_Image_Sets --> Shuffle --> Test-Train_Split</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEWiO1SIF7AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Handling directories\n",
        "HGG_dir_list = next(os.walk('./BRATS2015/BRATS2015_Training/HGG/'))[1]\n",
        "for i in range(len(HGG_dir_list)):\n",
        "  HGG_dir_list[i]='./BRATS2015/BRATS2015_Training/HGG/'+HGG_dir_list[i]\n",
        "\n",
        "LGG_dir_list = next(os.walk('./BRATS2015/BRATS2015_Training/LGG/'))[1]\n",
        "for i in range(len(LGG_dir_list)):\n",
        "  LGG_dir_list[i]='./BRATS2015/BRATS2015_Training/LGG/'+LGG_dir_list[i]\n",
        "\n",
        "\n",
        "completelist = HGG_dir_list + LGG_dir_list\n",
        "\n",
        "## shuffles in place\n",
        "np.random.shuffle(completelist) \n",
        "\n",
        "\n",
        "## Train-Test dictionary\n",
        "train_percentage=0.8\n",
        "\n",
        "l0=0\n",
        "l1=int(len(completelist)*train_percentage)+1\n",
        "l2=len(completelist)\n",
        "\n",
        "print(l1)\n",
        "print(l2-l1)\n",
        "\n",
        "partition={'train':np.empty((l1,5),dtype=object),'test':np.empty((l2-l1,5),dtype=object)}\n",
        "\n",
        "for i in range(l1):\n",
        "  a = next(os.walk(completelist[i]))[1]\n",
        "  a.sort()\n",
        "  for j in range(5):\n",
        "    partition['train'][i][j]=completelist[i]+ '/'+a[j]+'/'+a[j]+'.mha'\n",
        "    \n",
        "for i in range(l1,l2):\n",
        "  a = next(os.walk(completelist[i]))[1]\n",
        "  a.sort()\n",
        "  for j in range(5):\n",
        "    partition['test'][i-l1][j]=completelist[i]+ '/'+a[j]+'/'+a[j]+'.mha'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgxhiUQAstsI",
        "colab_type": "text"
      },
      "source": [
        "<h2>mha --> numpy_3D_matrix</h2> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KUOqNynF-Hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numpy_3D_matrix(ID):\n",
        "  return(sitk.GetArrayFromImage(sitk.ReadImage(ID)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVRXJHwwA4WZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Example\n",
        "\n",
        "ct_scan = numpy_3D_matrix(partition['test'][4][4])\n",
        "np.shape(ct_scan)\n",
        "plt.imshow(ct_scan[50,:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v82ekFBktpb9",
        "colab_type": "text"
      },
      "source": [
        "<h1>MODEL</h1><h2>Pre-processing + U-Net + more filters + data augmentation + dice-loss + segmented image classification</h2>Refernces: https://arxiv.org/pdf/1802.10508v1.pdf , https://arxiv.org/pdf/1701.03056.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMcgnYgEcSge",
        "colab_type": "text"
      },
      "source": [
        "<h2>Loss</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4dHyeRMcTG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1.):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coefficient_loss(y_true, y_pred):\n",
        "    return -dice_coefficient(y_true, y_pred)\n",
        "\n",
        "\n",
        "def weighted_dice_coefficient(y_true, y_pred, axis=(-3, -2, -1), smooth=0.00001):\n",
        "    return K.mean(2. * (K.sum(y_true * y_pred,\n",
        "                              axis=axis) + smooth/2)/(K.sum(y_true,\n",
        "                                                            axis=axis) + K.sum(y_pred,\n",
        "                                                                               axis=axis) + smooth))\n",
        "\n",
        "\n",
        "def weighted_dice_coefficient_loss(y_true, y_pred):\n",
        "    return -weighted_dice_coefficient(y_true, y_pred)\n",
        "\n",
        "\n",
        "def label_wise_dice_coefficient(y_true, y_pred, label_index):\n",
        "    return dice_coefficient(y_true[:, label_index], y_pred[:, label_index])\n",
        "\n",
        "\n",
        "def get_label_dice_coefficient_function(label_index):\n",
        "    f = partial(label_wise_dice_coefficient, label_index=label_index)\n",
        "    f.__setattr__('__name__', 'label_{0}_dice_coef'.format(label_index))\n",
        "    return f\n",
        "\n",
        "\n",
        "dice_coef = dice_coefficient\n",
        "dice_coef_loss = dice_coefficient_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ89xxKTeKo5",
        "colab_type": "text"
      },
      "source": [
        "<h2>Parts to create model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j74nRSb4uiSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_convolution_block(input_layer, \n",
        "                             n_filters, \n",
        "                             batch_normalization=False, \n",
        "                             kernel=(3, 3, 3), \n",
        "                             activation=None,\n",
        "                             padding='same',\n",
        "                             strides=(1, 1, 1), \n",
        "                             instance_normalization=False):\n",
        "    \n",
        "    \n",
        "    layer = Conv3D(n_filters, kernel, padding=padding, strides=strides)(input_layer)\n",
        "    \n",
        "    if batch_normalization:\n",
        "        layer = BatchNormalization(axis=1)(layer)\n",
        "    \n",
        "    elif instance_normalization:\n",
        "        layer = InstanceNormalization(axis=1)(layer)\n",
        "    \n",
        "    if activation is None:\n",
        "        return Activation('relu')(layer)\n",
        "    \n",
        "    else:\n",
        "        return activation()(layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqxWUVLXLSYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_level_output_shape(n_filters, depth, pool_size, image_shape):\n",
        "    output_image_shape = np.asarray(np.divide(image_shape, np.power(pool_size, depth)), dtype=np.int32).tolist()\n",
        "    return tuple([None, n_filters] + output_image_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n32AKoRsLU32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_up_convolution(n_filters, pool_size, kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
        "                       deconvolution=False):\n",
        "    if deconvolution:\n",
        "        return Deconvolution3D(filters=n_filters, kernel_size=kernel_size,\n",
        "                               strides=strides)\n",
        "    else:\n",
        "        return UpSampling3D(size=pool_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aSm9qA5LYx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_localization_module(input_layer, n_filters):\n",
        "    convolution1 = create_convolution_block(input_layer, n_filters)\n",
        "    convolution2 = create_convolution_block(convolution1, n_filters, kernel=(1, 1, 1))\n",
        "    return convolution2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5RGREpYLZqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_up_sampling_module(input_layer, n_filters, size=(2, 2, 2)):\n",
        "    up_sample = UpSampling3D(size=size)(input_layer)\n",
        "    convolution = create_convolution_block(up_sample, n_filters)\n",
        "    return convolution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkuilzBPLcKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_context_module(input_layer, n_level_filters, dropout_rate=0.3, data_format=\"channels_first\"):\n",
        "    convolution1 = create_convolution_block(input_layer=input_layer, n_filters=n_level_filters)\n",
        "    dropout = SpatialDropout3D(rate=dropout_rate, data_format=data_format)(convolution1)\n",
        "    convolution2 = create_convolution_block(input_layer=dropout, n_filters=n_level_filters)\n",
        "    return convolution2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhrXPtfBLsU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_convolution_block = partial(create_convolution_block, activation=LeakyReLU, instance_normalization=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IOkH5fic42A",
        "colab_type": "text"
      },
      "source": [
        "<h2>Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XZYLHayT7CB",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f5702e1c095b99de32251967dab69ddab341736b/6-Figure4-1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Limp6CLw8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MODEL(input_shape=(4, 1, 128, 128),\n",
        "                      n_base_filters=16,\n",
        "                      depth=5, \n",
        "                      dropout_rate=0.3,\n",
        "                      n_segmentation_levels=3,\n",
        "                      n_labels=4, \n",
        "                      optimizer=Adam, \n",
        "                      initial_learning_rate=5e-4,\n",
        "                      loss_function=weighted_dice_coefficient_loss, \n",
        "                      activation_name=\"sigmoid\"):    \n",
        "    \n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    current_layer = inputs\n",
        "    level_output_layers = list()\n",
        "    level_filters = list()\n",
        "    for level_number in range(depth):\n",
        "        n_level_filters = (2**level_number) * n_base_filters\n",
        "        level_filters.append(n_level_filters)\n",
        "\n",
        "        if current_layer is inputs:\n",
        "            in_conv = create_convolution_block(current_layer, n_level_filters)\n",
        "        else:\n",
        "            in_conv = create_convolution_block(current_layer, n_level_filters, strides=(2, 2, 2))\n",
        "\n",
        "        context_output_layer = create_context_module(in_conv, n_level_filters, dropout_rate=dropout_rate)\n",
        "\n",
        "        summation_layer = Add()([in_conv, context_output_layer])\n",
        "        level_output_layers.append(summation_layer)\n",
        "        current_layer = summation_layer\n",
        "\n",
        "    segmentation_layers = list()\n",
        "    for level_number in range(depth - 2, -1, -1):\n",
        "        up_sampling = create_up_sampling_module(current_layer, level_filters[level_number])\n",
        "        concatenation_layer = concatenate([level_output_layers[level_number], up_sampling], axis=1)\n",
        "        localization_output = create_localization_module(concatenation_layer, level_filters[level_number])\n",
        "        current_layer = localization_output\n",
        "        if level_number < n_segmentation_levels:\n",
        "            segmentation_layers.insert(0, create_convolution_block(current_layer, n_filters=n_labels, kernel=(1, 1, 1)))\n",
        "\n",
        "    output_layer = None\n",
        "    for level_number in reversed(range(n_segmentation_levels)):\n",
        "        segmentation_layer = segmentation_layers[level_number]\n",
        "        if output_layer is None:\n",
        "            output_layer = segmentation_layer\n",
        "        else:\n",
        "            output_layer = Add()([output_layer, segmentation_layer])\n",
        "\n",
        "        if level_number > 0:\n",
        "            output_layer = UpSampling3D(size=(2, 2, 2))(output_layer)\n",
        "\n",
        "    activation_block = Activation(activation_name)(output_layer)\n",
        "    survival_block = Activation(\"linear\")(summation_layer)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[activation_block,survival_block])\n",
        "    model.compile(optimizer=optimizer(lr=initial_learning_rate), loss=loss_function)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGOgGJERL7I-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MODEL(input_shape=(4, 1, 192, 160), n_base_filters=6, depth=5, dropout_rate=0.3,\n",
        "                      n_segmentation_levels=3, n_labels=3, optimizer=Adam, initial_learning_rate=5e-4,\n",
        "                      loss_function=weighted_dice_coefficient_loss, activation_name=\"sigmoid\")\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM5IBzNw-SCV",
        "colab_type": "text"
      },
      "source": [
        "<h2>Training MODEL</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ1vV_QC-SeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'dim': (160,192,160),\n",
        "          'batch_size': 2,\n",
        "          'n_classes': 3,\n",
        "          'n_channels': 4,\n",
        "          'shuffle': True}\n",
        "\n",
        "training_generator = \n",
        "validation_generator = \n",
        "\n",
        "cb_1=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
        "cb_2=keras.callbacks.ModelCheckpoint(filepath=weights.{epoch:02d}-{val_loss:.2f}.hdf5, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "results = model.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                   epochs=2, \n",
        "                   nb_worker=4,\n",
        "                   callbacks=[cb_1,cb_2])\n",
        "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbWZ2RCO_RJx",
        "colab_type": "text"
      },
      "source": [
        "<h2> Testing Model </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjtrQvjq_DJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'dim': (160,192,160),\n",
        "          'batch_size': 2,\n",
        "          'n_classes': 3,\n",
        "          'n_channels': 4,\n",
        "          'shuffle': False}\n",
        "\n",
        "validation_generator = DataGenerator(partition['holdout'], labels, **params)\n",
        "\n",
        "prediction = model.predict_generator(generator=validation_generator)\n",
        "\n",
        "prediction"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}