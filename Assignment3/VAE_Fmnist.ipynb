{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE Fmnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CxG3Kg0R66YH"
      },
      "source": [
        "<h1> Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2hxdxhlRGCd0"
      },
      "source": [
        "Make sure to download fmnist in data/fashion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OBoNhhCL6Y-G",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "fmnist = input_data.read_data_sets(\"/data/fashion\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PAyYCnR1698m"
      },
      "source": [
        "<h1> Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uEzPI9DB6ysA",
        "colab": {}
      },
      "source": [
        "lr_param = 0.001\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "image_dimension = 784\n",
        "neural_network_dimension = 512\n",
        "\n",
        "latent_variable_dimension = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CLgjfCPk7-Se"
      },
      "source": [
        "<h3>Initialize</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUlfWkjD7cfu",
        "colab": {}
      },
      "source": [
        "def xavier(in_shape):\n",
        "  val = tf.random_normal(shape = in_shape, stddev = 1/tf.sqrt(in_shape[0]/2.0))\n",
        "  return(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5SFgLElT8HPe"
      },
      "source": [
        "<h3> Weights and Biases</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MrTITfgj8K5-",
        "colab": {}
      },
      "source": [
        "Weight = { \"weight_matrix_encoder_hidden\" : tf.Variable(xavier([image_dimension, neural_network_dimension])),\n",
        "          \"weight_mean_hidden\" : tf.Variable(xavier([neural_network_dimension, latent_variable_dimension])),\n",
        "          \"weight_std_hidden\" : tf.Variable(xavier([neural_network_dimension, latent_variable_dimension])),\n",
        "          \"weight_matrix_decoder_hidden\" : tf.Variable(xavier([latent_variable_dimension, neural_network_dimension])),\n",
        "          \"weight_decoder\" : tf.Variable(xavier([neural_network_dimension, image_dimension])),\n",
        "         }\n",
        "Bias = {\"bias_matrix_encoder_hidden\" : tf.Variable(xavier([neural_network_dimension])),\n",
        "          \"bias_mean_hidden\" : tf.Variable(xavier([latent_variable_dimension])),\n",
        "          \"bias_std_hidden\" : tf.Variable(xavier([latent_variable_dimension])),\n",
        "          \"bias_matrix_decoder_hidden\" : tf.Variable(xavier([neural_network_dimension])),\n",
        "          \"bias_decoder\" : tf.Variable(xavier([image_dimension])),\n",
        "         }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ufqE13k_-ROd"
      },
      "source": [
        "<h1> Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sYzf9TYH8CkX",
        "colab": {}
      },
      "source": [
        "image_x = tf.placeholder(tf.float32, shape = [None, image_dimension])\n",
        "\n",
        "Encoder_layer = tf.add(tf.matmul(image_x, Weight[\"weight_matrix_encoder_hidden\"]),Bias[\"bias_matrix_encoder_hidden\"])\n",
        "Encoder_layer = tf.nn.tanh(Encoder_layer)\n",
        "\n",
        "Mean_layer = tf.add(tf.matmul(Encoder_layer, Weight[\"weight_mean_hidden\"]), Bias[\"bias_mean_hidden\"])\n",
        "Standard_deviation_layer = tf.add(tf.matmul(Encoder_layer, Weight[\"weight_std_hidden\"]), Bias[\"bias_std_hidden\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ysHPUbKK_HXG"
      },
      "source": [
        "<h1> Reparameterization Trick"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qxwi0VRH_J52",
        "colab": {}
      },
      "source": [
        "epsilon = tf.random_normal(tf.shape(Standard_deviation_layer), dtype = tf.float32, mean= 0.0, stddev = 1.0)\n",
        "latent_layer = Mean_layer*tf.exp(0.5*Standard_deviation_layer)*epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RC6rTeBy_Ky9"
      },
      "source": [
        "<h1> Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FxEHmF3K_MGu",
        "colab": {}
      },
      "source": [
        "Decoder_hidden = tf.add(tf.matmul(latent_layer, Weight[\"weight_matrix_decoder_hidden\"]),Bias[\"bias_matrix_decoder_hidden\"])\n",
        "Decoder_hidden = tf.nn.tanh(Decoder_hidden)\n",
        "\n",
        "Decoder_output_layer = tf.add(tf.matmul(Decoder_hidden, Weight[\"weight_decoder\"]), Bias[\"bias_decoder\"])\n",
        "Decoder_output_layer = tf.nn.sigmoid(Decoder_output_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qnzPjm9aEJsw"
      },
      "source": [
        "<h2> Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTcvj0PREA1J",
        "colab": {}
      },
      "source": [
        "def loss_function(original_image, reconstructed_image):\n",
        "  \n",
        "  # Reconstruction Loss\n",
        "  data_fidelity_loss = original_image * tf.log(1e-10 + reconstructed_image) + (1-original_image)*tf.log(1e-10 + 1- reconstructed_image)\n",
        "  data_fidelity_loss = -tf.reduce_sum(data_fidelity_loss, 1)\n",
        "  \n",
        "  # KL Divergence loss\n",
        "  KL_div_loss = 1+Standard_deviation_layer - tf.square(Mean_layer) - tf.exp(Standard_deviation_layer)\n",
        "  KL_div_loss = -0.5 * tf.reduce_sum(KL_div_loss, 1)\n",
        "  \n",
        "  alpha = 1\n",
        "  beta = 1\n",
        "  network_loss = tf.reduce_mean(alpha*data_fidelity_loss + beta*KL_div_loss)\n",
        "  \n",
        "  return(network_loss)\n",
        "\n",
        "loss_value = loss_function(image_x, Decoder_output_layer)\n",
        "\n",
        "optimizer =tf.train.RMSPropOptimizer(lr_param).minimize(loss_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uPAwCVTwGbDE"
      },
      "source": [
        "<h2> Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fIKy9dYhGZ_k",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "sess.run(init)\n",
        "\n",
        "for i in range(epochs):\n",
        "    x_batch,_ = fmnist.train.next_batch(batch_size)\n",
        "    _, loss = sess.run([optimizer, loss_value], feed_dict = {image_x: x_batch})\n",
        "  \n",
        "    \n",
        "  print(\"Loss is {0} at iteration {1}\".format(loss, i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XMvs82ngHpyD"
      },
      "source": [
        "<h2> Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWwFkK0LHrKr",
        "colab": {}
      },
      "source": [
        "noise_x = tf.placeholder(tf.float32, shape = [None, latent_variable_dimension])\n",
        "\n",
        "Decoder_hidden = tf.add(tf.matmul(latent_layer, Weight[\"weight_matrix_decoder_hidden\"]),Bias[\"bias_matrix_decoder_hidden\"])\n",
        "Decoder_hidden = tf.nn.tanh(Decoder_hidden)\n",
        "\n",
        "Decoder_output_layer = tf.add(tf.matmul(Decoder_hidden, Weight[\"weight_decoder\"]), Bias[\"bias_decoder\"])\n",
        "Decoder_output_layer = tf.nn.sigmoid(Decoder_output_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eEx0jxIaICQN"
      },
      "source": [
        "<h3> Output Visualization</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "deGMX20qIHth",
        "colab": {}
      },
      "source": [
        "n = 20\n",
        "x_axis = np.linspace(-2, 2, n)\n",
        "y_axis = np.linspace(-2, 2, n)\n",
        "\n",
        "canvas = np.empty((28 * n, 28 * n))\n",
        "for i, yi in enumerate(x_axis):\n",
        "    for j, xi in enumerate(y_axis):\n",
        "        z_mu = np.array([[xi, yi]] * batch_size)\n",
        "        x_mean = sess.run(Decoder_output_layer, feed_dict={noise_input: z_mu})\n",
        "        canvas[(n - i - 1) * 28:(n - i) * 28, j * 28:(j + 1) * 28] = \\\n",
        "        x_mean[0].reshape(28, 28)\n",
        "\n",
        "plt.figure(figsize=(8, 10))\n",
        "Xi, Yi = np.meshgrid(x_axis, y_axis)\n",
        "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}