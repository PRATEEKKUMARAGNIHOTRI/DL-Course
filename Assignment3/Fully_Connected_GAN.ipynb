{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fully Connected GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKjOnOxwmUoV",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pickleshare import pickle\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('data/fashion', source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')\n",
        "\n",
        "data_batch=mnist.train.next_batch(1)\n",
        "data_batch=data_batch[0].reshape((28,28))\n",
        "data_batch=np.array(data_batch)\n",
        "print(data_batch.shape)\n",
        "plt.imshow(data_batch)\n",
        "plt.show()\n",
        "print(\"Size of:\")\n",
        "print(\"Training-set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
        "print(\"- Validation-set:\\t{}\".format(len(mnist.test.labels)))\n",
        "print(\"- Test-set:\\t\\t{}\".format(len(mnist.validation.labels)))\n",
        "\n",
        "#defining placeholder variables for inputs\n",
        "\n",
        "def placeholder(img_dim,z_dim):\n",
        "\timg_ph=tf.placeholder(tf.float32,(None,img_dim),name=\"image_data\")\n",
        "\tz_ph=tf.placeholder(tf.float32,(None,z_dim),name=\"hidden_data\")\n",
        "\treturn img_ph,z_ph\n",
        "\n",
        "\n",
        "def generator(z,out_dim,n_units,reuse):\n",
        "  with tf.variable_scope('generator',reuse=reuse):\n",
        "    #the hidden layer with n_units\n",
        "    h1=tf.layers.dense(z,n_units,activation=tf.nn.relu)\n",
        "    h1=tf.layers.dense(h1,n_units,activation=tf.nn.relu)\n",
        "    h1=tf.layers.dense(h1,n_units,activation=tf.nn.relu)\n",
        "    #output layer\n",
        "    logits=tf.layers.dense(h1,out_dim,activation=None)\n",
        "    out=tf.tanh(logits)\n",
        "    return out\n",
        "\n",
        "def discriminator(x,n_units,reuse=False):\n",
        "  with tf.variable_scope('discriminator',reuse=reuse):\n",
        "    #hidden layer with n_units\n",
        "    h1=tf.layers.dense(x,n_units,activation=tf.nn.relu)\n",
        "\n",
        "    h1=tf.layers.dense(h1,n_units,activation=tf.nn.relu)\n",
        "\n",
        "    h1=tf.layers.dense(h1,n_units,activation=tf.nn.relu)\n",
        "    #output layer\n",
        "    logits=tf.layers.dense(h1,1,activation=None)\n",
        "    out=tf.sigmoid(logits)\n",
        "    return out,logits\n",
        "\n",
        "\n",
        "input_size=784\n",
        "z_dim=50\n",
        "g_hidden=128\n",
        "d_hidden=128\n",
        "smoothing_param=0.1 # to keep the sigmoid betwen 0 and 0.9\n",
        "\n",
        "#graph initiation\n",
        "tf.reset_default_graph()\n",
        "real_x, input_z=placeholder(input_size,z_dim)\n",
        "#generator node\n",
        "g_model=generator(input_z,input_size,g_hidden,reuse=False)\n",
        "#discriminator node\n",
        "#1. for real data\n",
        "#2. for fake data(we'll use the same layer again )\n",
        "d_model_real,d_logit_real=discriminator(real_x,d_hidden,False)\n",
        "d_model_fake,d_logit_fake=discriminator(g_model,d_hidden,True)\n",
        "\n",
        "#loss functions\n",
        "#discriminator loss -> real loss,->fake loss\n",
        "d_real_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logit_real,labels=tf.ones_like(d_logit_real)*(1-smoothing_param)))\n",
        "d_fake_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logit_fake,labels=tf.zeros_like(d_logit_fake)))\n",
        "\n",
        "total_d_loss=d_real_loss+d_fake_loss\n",
        "\n",
        "#generator loss\n",
        "g_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logit_fake,labels=tf.ones_like(d_logit_fake)))\n",
        "\n",
        "#optimizer\n",
        "lr=0.0005\n",
        "t_var=tf.trainable_variables()\n",
        "d_var=[var for var in t_var if var.name.startswith('discriminator')]\n",
        "g_var=[var for var in t_var if var.name.startswith('generator')]\n",
        "\n",
        "d_optimizer=tf.train.AdamOptimizer(lr).minimize(total_d_loss,var_list=d_var)\n",
        "g_optimizer=tf.train.AdamOptimizer(lr).minimize(g_loss,var_list=g_var)\n",
        "\n",
        "\n",
        "#start training\n",
        "\n",
        "batch_size=128 #power of 2\n",
        "epochs=10 # i don't have a GPU\n",
        "samples=[] #to print some generated images\n",
        "loss=[] #need to calm my heart\n",
        "\n",
        "#save generator variables\n",
        "gen_saved = tf.train.Saver(var_list=g_var)\n",
        "k=3\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(epochs):\n",
        "    for ii in range(mnist.train.num_examples//batch_size):\n",
        "      batch=mnist.train.next_batch(batch_size) #takes a batch size of 128 random images\n",
        "      images=batch[0].reshape((batch_size,784))\n",
        "      images=images*2-1\n",
        "\n",
        "      #sample z\n",
        "      for iii in range(k):\n",
        "        z=np.random.uniform(-1,1,size=(batch_size,z_dim))\n",
        "        sess.run(d_optimizer,feed_dict={real_x:images,input_z:z})\n",
        "      \n",
        "      z=np.random.uniform(-1,1,size=(batch_size,z_dim))\n",
        "      sess.run(g_optimizer,feed_dict={input_z:z})\n",
        "\n",
        "    train_loss_d=sess.run(total_d_loss,{input_z:z,real_x:images})\n",
        "    train_loss_g=g_loss.eval({input_z:z})\n",
        "    print(\"Epoch {}/{}...\".format(i+1, epochs),\n",
        "                \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
        "                \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
        "    loss.append((train_loss_d,train_loss_g))\n",
        "    sample_z = np.random.uniform(-1, 1, size=(16, z_dim))\n",
        "    gen_samples=sess.run(generator(input_z,input_size,g_hidden,reuse=True),feed_dict={input_z:sample_z})\n",
        "    samples.append(gen_samples)\n",
        "    gen_saved.save(sess, './checkpoints/generator.ckpt')\n",
        "\n",
        "# Save training generator samples\n",
        "with open('train_samples.pkl', 'wb') as f:\n",
        "    pkl.dump(samples, f)\n",
        "\n",
        "\n",
        "def view_samples(epoch, samples):\n",
        "\tprint(\"i'm here\")\n",
        "\tfig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
        "\tfor ax, img in zip(axes.flatten(), samples[epoch]):\n",
        "\t\tax.xaxis.set_visible(True)\n",
        "\t\tax.yaxis.set_visible(True)\n",
        "\t\tim = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
        "\treturn fig, axes\n",
        "  \n",
        "with open('train_samples.pkl', 'rb') as f:\n",
        "    samples = pkl.load(f)\n",
        "\n",
        "print(\"calling\")\n",
        "print(len(samples))\n",
        "p=view_samples(-1,samples)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}